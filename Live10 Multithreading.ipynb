{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Architecture:        x86_64\nCPU op-mode(s):      32-bit, 64-bit\nByte Order:          Little Endian\nCPU(s):              8\nOn-line CPU(s) list: 0-7\nThread(s) per core:  2\nCore(s) per socket:  4\nSocket(s):           1\nNUMA node(s):        1\nVendor ID:           GenuineIntel\nCPU family:          6\nModel:               142\nModel name:          Intel(R) Core(TM) i5-8265U CPU @ 1.60GHz\nStepping:            11\nCPU MHz:             2854.352\nCPU max MHz:         3900.0000\nCPU min MHz:         400.0000\nBogoMIPS:            3600.00\nVirtualization:      VT-x\nL1d cache:           32K\nL1i cache:           32K\nL2 cache:            256K\nL3 cache:            6144K\nNUMA node0 CPU(s):   0-7\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d arch_capabilities\n"
     ]
    }
   ],
   "source": [
    "#Several ML libraries use multithreading and multiple cores like Scikit Learn, they use a library called joblib\n",
    "# Nympy and Scipy use a library called BLAS (basic linear algebra for subroutines) for achieving multi threading\n",
    "\n",
    "# Number of CPUs and Threads in Linux.\n",
    "# Refer: https://linux.die.net/man/1/lscpu\n",
    "!lscpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean of 100 Million observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(100000000,)\n"
     ]
    }
   ],
   "source": [
    "# Generate random 100MM data points \n",
    "import numpy as np\n",
    "n =100000000\n",
    "d = np.random.rand(n)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25.498055458068848\n0.5000022375926746\n"
     ]
    }
   ],
   "source": [
    "#runs on 1 core on the cpu\n",
    "import time\n",
    "def mean():\n",
    "\n",
    "  #Sum using for loops. We can use inbuilt NumPy Sum opeartion for better speed.\n",
    "  sum = 0\n",
    "  n=d.size\n",
    "  for i in range(n):\n",
    "    sum +=d[i]\n",
    "\n",
    "  #Mean\n",
    "  mean = sum/n\n",
    "  return mean\n",
    "\n",
    "\n",
    "#Time the execution\n",
    "start_time = time.time()\n",
    "m = mean() # compute mean of 100MM numbers.\n",
    "end_time = time.time()\n",
    "print (end_time-start_time) #25sec\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12.733050346374512\n0.5000022375930983\n"
     ]
    }
   ],
   "source": [
    "#Multi processing and Multi Core\n",
    "\n",
    "#refer image 1 on how multi threading can be used to compute the mean of 100mn nos\n",
    "# this is a trivially multi processing task\n",
    "\n",
    "#Refer: https://docs.python.org/3/library/multiprocessing.html\n",
    "\n",
    "#we need 2 processes (p1, p2), where each process runs on each core, we can use the no of cores present on the machine\n",
    "\n",
    "from multiprocessing import Process, Queue #Queue is provided by the multiprocessing library to communicate b/w different processes\n",
    "import math\n",
    "\n",
    "def mean_MP(s, e, q ): #s: start index, e: end index, q: queue\n",
    "\n",
    "  #Sum using for loops. We can use inbuilt NumPy Sum opeartion for better speed.\n",
    "  sum = 0\n",
    "  for i in range(s,e+1):\n",
    "    sum +=d[i]\n",
    "\n",
    "  #Mean\n",
    "  mean = sum/(e-s+1)\n",
    "  q.put(mean) #enqueue the mean value\n",
    "  return \n",
    "\n",
    "n1 = math.floor(n/2)\n",
    "\n",
    "#Queues follow a FIFO approach, they can be entered from the start (enqueue) and removed from the start (dequeue)\n",
    "#compute mean of half of the nos in core 1 in a process 1 and enqueue the mean m1 in the queue\n",
    "#compute mean of rest of half of the nos in core 2 in a process 2 and enqueue the mean m2 in the queue\n",
    "#refer image 3\n",
    "\n",
    "#Queues are thread and process safe.\n",
    "# This means that if process 1 is trying to insert m1 and process 2 is trying to insert m2 into the queue at the same time - Inter process communication\n",
    "# But the Queue in multiprocessin lib is written in such a way that multiple processes can write to the queue, without worrying about multiple processes writing to the queue\n",
    "#add mean of process 1 to the queue\n",
    "#add mean of process 2 to the queue\n",
    "#queue is a shared resource to which processes can add data\n",
    "\n",
    "q = Queue() #Queues are thread and process safe. For communicating between processes and threads.\n",
    "\n",
    "p1 = Process(target=mean_MP, args=(0, n1,q ))  #target is the function which we wnt to execute, args: args to the function\n",
    "p2 = Process(target=mean_MP, args=(n1+1,n-1, q)) \n",
    "\n",
    "\n",
    "#Time the execution\n",
    "start_time = time.time()\n",
    "\n",
    "#The execution of this whole process is also carried out by a Process which we can call a Parent Process\n",
    "# This parent process has 2 child processes p1 and p2\n",
    "# All the Processes have data in their respective memory location and Processes also have some code\n",
    "# The parent process say has d (dataset=100mn) in memory and also some code\n",
    "# When the parent process runs, it spawns the 2 child processes p1 and p2 and it copies the variables/data (d) in the child processes p1 and p2\n",
    "# each child process has its own distinct memory, they don't know each other's memory\n",
    "# Each child process is execution the function mean_MP\n",
    "# Child processes communicate via the Queue - they read/write into the queue\n",
    "# p1 will process data from 0 to n1\n",
    "#p2 will process data from n1+1 to n-1\n",
    "#refer image 3\n",
    "# the parent process will be running on 1 of the cores of the computer\n",
    "# whenever the parent spawns achild process, the process gets assigned (scheduled) to a core in the computer by the OS\n",
    "# This process of assigning a process along with its data and code to a core is called Process Scheduling\n",
    "#There is some overhead or additional time taken by the OS to spawn a process and do Process scheduling\n",
    "# While the 2 child processes are running the parent process sleeps and wakes up once p1 and p2 are completed\n",
    "\n",
    "p1.start() # process 1 starts \n",
    "p2.start() # process 2 starts \n",
    "\n",
    "p1.join() # Parent process will Wait till p1 finishes\n",
    "p2.join() \n",
    "\n",
    "m=0;\n",
    "while not q.empty():\n",
    "     m += q.get() #get hings from the queue until there are items from the queue m = m1 + m2\n",
    "\n",
    "m /= 2; #take average\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print (end_time-start_time) #takes nearly half time  (includes OS time overhead)\n",
    "print(m)\n",
    "\n",
    "#In Multiprocessing, the data of the parent process is copied to the child processes, which adds to the memory overhead, but there are optimizations like the Queue like datastructers which can be efficient to store memory in a common ds to be accessible by the parent and the children. This will be at the cost of accessing these DS from the parent and child processes. Default MP duplicates memory in parent and children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20.910609006881714\n0.5000022375930983\n"
     ]
    }
   ],
   "source": [
    "#Multi Threading\n",
    "\n",
    "# CPU bound job: Jbbs which take a lot of time of the CPU eg: Numerical computations. These types of jobs are readily available to the CPU from RAM and CPU cache (registers)\n",
    "# I/O Bound job: To run a type of job, where we need to send some data from Disk(I/O device) to RAM and then RAM to CPU and it takes more time in the transfer to the CPU is I/O Bound (Numerical jobs can also sometimes be I/O bound, incase when there is a large dataset, because they need to read data)\n",
    "\n",
    "# In each core, there can only be 1 process at a time having its own memory and code, say for instance the code while executing requires some extrnal intervention say it needs to read data from the disk, at this time the CPU core will be idle, to utilize this idle time, Intel introduced the idea of multithreading, each process in the core will have 2 threads sharing the memory and code. When 1 thread is idle, the next will execute. This is the concept of Multithreading. Threads share the sae data between processes. They are lightweight and do faster context switching\n",
    "\n",
    "# Context switching is the time taken by the parent process to spawn child processes and copy its data into the child processes and assign each process to a CPU core\n",
    "# In threads, both the threads are on the same core sharing the same memory, it just needs to change the code which is being executed. Therefore in multi threading context switching is significantly faster. Multi threading is significantly useful if some threads are CPU bound and some are I/O bound\n",
    "\n",
    "#Refer: https://docs.python.org/3/library/threading.html\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "means = [0,0];\n",
    "\n",
    "def mean_MT(s, e, threadNum ):\n",
    "\n",
    "  #Sum using for loops. We can use inbuilt NumPy Sum opeartion for better speed.\n",
    "  sum = 0\n",
    "  for i in range(s,e+1):\n",
    "    sum +=d[i]\n",
    "\n",
    "  #Mean\n",
    "  mean = sum/(e-s+1) \n",
    "\n",
    "  #compute mean of 0th thread and add to mean[0]\n",
    "  #compute mean of 1st thread and add to mean[1]\n",
    "  means[threadNum] = mean; # means is a shared varibale between the threads can be accessed by both - instead of Queue in case of multi processing\n",
    "  #refer image 4\n",
    "\n",
    "  return \n",
    "\n",
    "n1 = math.floor(n/2)\n",
    "\n",
    "t1 = Thread(target=mean_MT, args=(0, n1,0 ))  #syntax simiar to Process, Third param is the thread number\n",
    "t2 = Thread(target=mean_MT, args=(n1+1,n-1,1)) # Third param is the thread number\n",
    "\n",
    "#Time the execution\n",
    "start_time = time.time()\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "t1.join() # Wait till t1 finishes\n",
    "t2.join() \n",
    "\n",
    "m = (means[0]+means[1])/2 #compute avg\n",
    "    \n",
    "end_time = time.time()\n",
    "print (end_time-start_time)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not much difference with the 1 process 1 thread computation and 1 process 2 threads computation\n",
    "This is because in the case with 1 process 2 threads the data d is already in the memory - no need to fetch any data, both the threads are CPU bound, no I/O needed, therefore the advantage of multi threading is very low in this case\n",
    "\n",
    "There is a problem called GIL with Python which slows down processes of multithreading. Python came up after multithreading was mainlly introduced\n",
    "\n",
    "Global Interpreter Lock -> worst part of Python multithreading\n",
    "Say there are 2 threads T1 and T2 sharing the same memory and code under the same process\n",
    "They both use the same Python interpreter\n",
    "\n",
    "GIL says that, if T1 thread is using the Python Interpreter to execute the code (same or different code), T2 can't use the python interpreter to execute its code\n",
    "This is because they are under the same process so they have to share the Python Interpreter between them\n",
    "T1 has to relinquish (give up) the control of the python interpreter before T2 can execute any of its code\n",
    "If T1 is waiting for some I/O at that time T1 will give up its Python Interpreter and T2 will acquire it\n",
    "This is the principle of GIL, when T1 is using the interpreter it is locking control on the Interpreter such that others can't access it\n",
    "This is one of the biggest problems in Python multithreaded code\n",
    "refer image 5\n",
    "\n",
    "The reason Python does this is because Python's memory Management is not thread safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To write multi processing or multi threading code we can use Process, Thread and Queues, but they are very low level functionality given by Python\n",
    "\n",
    "We can instead use a library called Joblib which is extensively used by python libs like Scikit learn for parallizing their code\n",
    "if we are not sure about writing the multi processing or multi threading code using Process, Thread and Queues, we can instead use Joblift which provides easy mechanisms\n",
    "\n",
    "One of the most used eg of Joblib is paralellizing for loops, bcoz for loops tae most time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jobib\n",
    "- Simple parallel computing in python (for loops)\n",
    "- Disk caching of function outputs\n",
    "- Widely used by Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "________________________________________________________________________________\n[Memory] Calling square...\nsquare(array([[0., 0., 1.],\n       [1., 1., 1.],\n       [4., 2., 1.]]))\n___________________________________________________________square - 0.0s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "#- Disk caching of function outputs\n",
    "#Transparent and fast disk-caching of output value\n",
    "# Refer: https://joblib.readthedocs.io/en/latest/\n",
    "from joblib import Memory\n",
    "cachedir = './' #caching dir\n",
    "mem = Memory(cachedir) #create some memory loc in the cache dir\n",
    "\n",
    "import numpy as np\n",
    "a = np.vander(np.arange(3)).astype(np.float) #create some var a\n",
    "square = mem.cache(np.square) # we want to use the np.square fn as part of the memory cache, ie whenever we compute square we want to cache it\n",
    "b = square(a) #square is executed on matrix a and the output b is cached in the memory location provided\n",
    "\n",
    "#joblib stores the mapping of a -> np.square -> b in a dict like structure\n",
    "# when we again call the same function, joblib checks if we have this already created, if we have it then don't reexecute it and fetch it directly\n",
    "# joblib is caching function output values, storing it in disk, thereby storing it for further evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = square(a)\n",
    "# The above call did not trigger an evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "21.691550493240356\n"
     ]
    }
   ],
   "source": [
    "# Simple Parallel programming for Loops\n",
    "# Refer: https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html\n",
    "\n",
    "import time\n",
    "from math import sqrt # inbuilt fucntion\n",
    "\n",
    "def f(i):\n",
    "    \n",
    "    # some computations  that take time, can do any computation which takes time\n",
    "    x=10000\n",
    "    p =1;\n",
    "    for j in range(x):\n",
    "        for k in range(j):\n",
    "            p *= k\n",
    "    \n",
    "    return sqrt(i ** 2);\n",
    "\n",
    "# Find sqrt of first n numbers\n",
    "n=10;\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(n): #call function 10 times with 10 different inputs\n",
    "    f(i)\n",
    "\n",
    "end_time = time.time()\n",
    "print (end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10.973873138427734\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "a = Parallel(n_jobs=2)(delayed(f)(i) for i in range(n)) #run this job parallely sing multi processing - 2 processes, we want to run the function f, which takes arguments from i from i = 0 to 9 (n=10), ie we want to run f(i) where i = 0-9 parallely with 2 processes\n",
    "\n",
    "# Why we need delayed(): https://stackoverflow.com/questions/42220458/what-does-the-delayed-function-do-when-used-with-joblib-in-python\n",
    "# Delayed waits for the the i values from 0-9 to be ready before executing f(i) parallely\n",
    "\n",
    "end_time = time.time() #nearly 50% time saver\n",
    "print (end_time-start_time)\n",
    "\n",
    "#when we have nested for loops, we can use this structure\n",
    "# jobblib expects that the function f(i) to execute is independent, one output does not depend on other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19.127604246139526\n"
     ]
    }
   ],
   "source": [
    "# Multi threading: GIL is an issue\n",
    "start_time = time.time()\n",
    "\n",
    "a = Parallel(n_jobs=2,prefer=\"threads\")(delayed(f)(i ** 2) for i in range(n)) #1 process 2 threads, prefer=\"Threads\"\n",
    "\n",
    "end_time = time.time() #no significant improvement bcoz we have CPU bound jobs and Python's GIL issue\n",
    "print (end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7.556748390197754\n"
     ]
    }
   ],
   "source": [
    "# 6 jobs\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "a = Parallel(n_jobs=6)(delayed(f)(i ** 2) for i in range(n)) \n",
    "\n",
    "# Why we need dealyed(): https://stackoverflow.com/questions/42220458/what-does-the-delayed-function-do-when-used-with-joblib-in-python\n",
    "\n",
    "end_time = time.time() #includes OS time overhead\n",
    "print (end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiprocessing can be efficiently used in python. Multithreading may not be efficient if jobs are CPU bound and the GIL issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib Useful in paralleizing Matrix and vector processing and data pre processing also in building ML models like Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "When the number of processes/jobs exceed the no of cores, at that time the OS will handle process scheduling such that all processes are orchestrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "When the file is very large in the disk, break the file up into 2 and assign each file path to a process to execute\n",
    "refer image 6 "
   ]
  }
 ]
}